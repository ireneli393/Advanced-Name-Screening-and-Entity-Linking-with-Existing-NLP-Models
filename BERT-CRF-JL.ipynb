{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"u68iDU_rtqB9"},"outputs":[],"source":["!pip install -U pytorch-crf\n","!pip install -U torchvision\n","!pip install -U transformers[torch]\n","!pip install -U scikit-learn\n","!pip install seqeval\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkEpALxPtqB_"},"outputs":[],"source":["import torch\n","import json\n","import gc\n","import time\n","import pandas as pd\n","import numpy as np\n","import torch.nn as nn\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from torchcrf import CRF\n","#from datasets import load_metric, Dataset\n","from transformers import (\n","    pipeline,\n","    AutoConfig,\n","    AutoTokenizer,\n","    AutoModelForTokenClassification,\n","    DataCollatorForTokenClassification,\n","    Trainer,\n","    TrainingArguments,\n","    PreTrainedModel,\n","    PretrainedConfig,\n","    BertModel,\n","    BertConfig,\n","    BertForTokenClassification\n",")\n","from datasets import load_dataset, load_metric, Dataset, DatasetDict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvZ9gfCItqB_","outputId":"1213e752-b761-4ca0-b1b4-c4cdd98734f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU is available.\n"]}],"source":["#check if cuda is available\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")  # Use GPU\n","    print(\"GPU is available.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FxSYZIEtqCA","outputId":"0a2ef8fb-a1cb-40d8-9267-570223c0e598"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","0\n"]}],"source":["torch.cuda.empty_cache()\n","print(torch.cuda.memory_allocated(device))\n","print(torch.cuda.max_memory_allocated(device))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYJ5AyXBtqCA"},"outputs":[],"source":["bert_name = \"dslim/bert-large-NER\"\n","tokenizer = AutoTokenizer.from_pretrained(bert_name)\n","config = AutoConfig.from_pretrained(bert_name)"]},{"cell_type":"markdown","metadata":{"id":"7jmWT3sftqCA"},"source":["# READ AND PREPROCESS DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_IH6f5itqCB"},"outputs":[],"source":["# read dataset and separate training set from testing set\n","def read_dataset(file):\n","    with open(file, \"r\") as json_file:\n","        data = json.load(json_file)\n","        train = data['train']\n","        test = data['test']\n","    return train, test\n","\n","file = \"label_data_train_test.json\"\n","train, test = read_dataset(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NotDdkantqCB"},"outputs":[],"source":["def encode_examples(text, entities):\n","    # Tokenize the text\n","    tokens = tokenizer(text, truncation=True, padding='max_length', max_length=512)\n","    word_ids = tokens.word_ids()\n","\n","    # Initialize labels with -100 for special tokens and padding\n","    labels = [-100] * len(tokens['input_ids'])\n","\n","\n","    previous_word_idx = None\n","\n","    labels_mask = []\n","\n","    for word_idx in word_ids:  # Set the special tokens to -100.\n","            if word_idx is None:\n","                labels_mask.append(False)\n","            # Only label the first token of a given word.\n","            elif word_idx != previous_word_idx:\n","                labels_mask.append(True)\n","            else:\n","                labels_mask.append(False)\n","\n","            previous_word_idx = word_idx\n","\n","\n","    for entity in entities:\n","        entity_type, start_char, end_char = entity\n","        # Convert character start and end positions to token start and end positions\n","        start_token = tokens.char_to_token(start_char)\n","        end_token = tokens.char_to_token(end_char - 1)  # Subtract 1 because the end index is inclusive\n","\n","        # If end_token is None, we move end_char to the right until we find a token\n","        if end_token is None and start_token is not None:\n","            while end_token is None and end_char > start_char:\n","                end_char -= 1  # Move left since we may have overshot the actual token\n","                end_token = tokens.char_to_token(end_char)\n","\n","        if start_token is None or end_token is None:\n","            continue  # Skip entities that couldn't be mapped to tokens\n","\n","        # Get label IDs based on entity type\n","        label_ids = {\n","            'PERSON': (3, 4),\n","            'ORG': (5, 6)\n","        }.get(entity_type, (0, 0))\n","\n","        b_label_id, i_label_id = label_ids\n","\n","        # Label the first token of the entity\n","        labels[start_token] = b_label_id\n","\n","        # Label subsequent tokens of the entity\n","        for i in range(start_token + 1, end_token + 1):\n","            labels[i] = i_label_id\n","\n","    # Now, set the non-special and non-padding tokens to 'O'\n","    for i, token_id in enumerate(tokens['input_ids']):\n","        if token_id not in [tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id]:\n","            if labels[i] == -100:  # Only if it's not already labeled\n","                labels[i] = 0\n","\n","    return {\"input_ids\": tokens['input_ids'], \"attention_mask\": tokens['attention_mask'], \"labels\": labels, 'label_mask': labels_mask}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"By9wf1fBtqCB"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","class NERDataset(Dataset):\n","    def __init__(self, texts, entities):\n","        self.texts = texts\n","        self.entities = entities\n","        self.encodings = [encode_examples(text, entity) for text, entity in zip(texts, entities)]\n","\n","    def __getitem__(self, idx):\n","        return self.encodings[idx]\n","\n","    def __len__(self):\n","        return len(self.texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KjrpKDj3tqCC","outputId":"e985f2a8-8032-4aca-c5ed-fe3ec1da84e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 951 ms, sys: 13.7 ms, total: 965 ms\n","Wall time: 969 ms\n"]}],"source":["%%time\n","# Create an instance of the dataset\n","all_train_texts = [item[0] for item in train]\n","all_train_entities = [item[1] for item in train]\n","train_dataset = NERDataset(all_train_texts, all_train_entities)\n","all_test_texts = [item[0] for item in test]\n","all_test_entities = [item[1] for item in test]\n","test_dataset = NERDataset(all_test_texts, all_test_entities)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPUu5iwFtqCC"},"outputs":[],"source":["class BERT_CRF(nn.Module):\n","    def __init__(self, dropout):\n","\n","        super().__init__()\n","        bert_config = BertConfig.from_pretrained(bert_name)\n","        bert_config.output_attentions = True\n","        bert_config.output_hidden_states = True\n","        num_labels = bert_config.num_labels\n","\n","        self.bert = BertModel.from_pretrained(bert_name, config = bert_config)\n","\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        self.linear = nn.Linear(self.bert.config.hidden_size, num_labels)\n","\n","        self.crf = CRF(num_labels, batch_first=True)\n","\n","    def forward(self, input_ids, attention_mask, labels_mask, labels = None):\n","\n","        last_hidden_layer = self.bert(input_ids=input_ids, attention_mask=attention_mask)[\n","            'last_hidden_state']\n","\n","        last_hidden_layer = self.dropout(last_hidden_layer)\n","\n","        logits = self.linear(last_hidden_layer)\n","\n","        batch_size = logits.shape[0]\n","\n","        output_tags = []\n","\n","        if labels is not None:\n","            loss = 0\n","\n","            for seq_logits, seq_labels, seq_mask in zip(logits, labels, labels_mask):\n","                # Index logits and labels using prediction mask to pass only the\n","                # first subtoken of each word to CRF.\n","                seq_logits = seq_logits[seq_mask].unsqueeze(0)\n","                seq_labels = seq_labels[seq_mask].unsqueeze(0)\n","\n","                if seq_logits.numel() != 0:\n","                    loss -= self.crf(seq_logits, seq_labels,\n","                                     reduction='token_mean')\n","\n","            return loss / batch_size\n","        else:\n","            for seq_logits, seq_mask in zip(logits, labels_mask):\n","                seq_logits = seq_logits[seq_mask].unsqueeze(0)\n","                if seq_logits.numel() != 0:\n","                    tags = self.crf.decode(seq_logits)\n","                else:\n","                    tags = [[]]\n","\n","                # Unpack \"batch\" results\n","\n","                output_tags.append(tags[0])\n","\n","            return output_tags"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmsOL_gUtqCC"},"outputs":[],"source":["batch_size = 10\n","batch_train_dataset = DataLoader(train_dataset, batch_size=batch_size)\n","batch_test_dataset = DataLoader(test_dataset, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YwpP7x4ytqCC","outputId":"f81391b9-1b12-4115-a6bd-6f89c78c3938"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 0\n","loss: 9.859933614730835\n","epoch 1\n","loss: 2.964125607162714\n","epoch 2\n","loss: 2.063571787904948\n","epoch 3\n","loss: 1.7061817408539355\n","epoch 4\n","loss: 1.4212387232109904\n","epoch 5\n","loss: 1.100999459857121\n","epoch 6\n","loss: 0.921869860845618\n","epoch 7\n","loss: 0.8531488277949393\n","CPU times: user 11min 9s, sys: 18.3 s, total: 11min 27s\n","Wall time: 11min 31s\n"]}],"source":["%%time\n","from sklearn.metrics import accuracy_score\n","\n","if __name__ == \"__main__\":\n","\n","    # Initialize the BERT-CRF model\n","    dropout = 0.5\n","    model = BERT_CRF(dropout).to(device)\n","\n","    # Training loop\n","    optimizer = torch.optim.AdamW(model.parameters(), lr = 2e-5)\n","    num_epochs = 8  # Adjust the number of training epochs as needed\n","    model.train()\n","    for epoch in range(num_epochs):\n","        print(f'epoch {epoch}')\n","        loss_num = 0\n","        for batch in batch_train_dataset:\n","            optimizer.zero_grad()\n","            # change the shape and make the batch size as the first dimension\n","            input_ids = torch.stack(batch[\"input_ids\"], dim=0).t().to(device)\n","            attention_mask = torch.stack(batch[\"attention_mask\"], dim=0).t().to(device)\n","            labels = torch.stack(batch[\"labels\"], dim=0).t().to(device)\n","            labels_mask = torch.stack(batch[\"label_mask\"], dim=0).t().to(device)\n","            loss = model(input_ids, attention_mask, labels_mask, labels)\n","            loss_num += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","        print('loss:',loss_num)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jC-rGC6ftqCC","outputId":"70ead82c-c503-406d-f53d-7534c77b8647"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 2.89 s, sys: 1.98 ms, total: 2.89 s\n","Wall time: 2.91 s\n"]}],"source":["%%time\n","def evaluate_model(model, eval_dataset, device):\n","    model.eval()\n","    true_labels = []  # Ground truth labels\n","    predicted_labels = []  # Predicted labels\n","\n","    with torch.no_grad():\n","        for batch in eval_dataset:\n","            input_ids = torch.stack(batch[\"input_ids\"], dim=0).t().to(device)\n","            attention_mask = torch.stack(batch[\"attention_mask\"], dim=0).t().to(device)\n","            labels_mask = torch.stack(batch[\"label_mask\"], dim=0).t().to(device)\n","\n","            # Make predictions\n","            predicted_tags = model(input_ids, attention_mask, labels_mask)\n","\n","            labels = torch.stack(batch[\"labels\"], dim=0).t().to(device)\n","\n","            masked_true_labels = [torch.tensor(label)[mask].tolist() for label, mask in zip(labels, labels_mask)]\n","\n","            true_labels.extend(masked_true_labels)\n","            predicted_labels.extend(predicted_tags)\n","    return (true_labels, predicted_labels)\n","\n","\n","true_labels, predicted_labels = evaluate_model(model, batch_test_dataset, device)"]},{"cell_type":"markdown","metadata":{"id":"s8t6qEPOtqCC"},"source":["# **Seqeval Overall Comparison Report on Test Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wj-mqzIAtqCC"},"outputs":[],"source":["#convert ids to ner\n","predicted_ner = [[config.id2label[label] for label in sample] for sample in predicted_labels]\n","true_ner = [[config.id2label[sample[j]] for j in range(len(sample))] for sample in true_labels]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9Ts5AEctqCC","outputId":"76b7cd6a-6d03-417e-b8e3-c7e43c80c538"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'precision': 0.7101167315175098, 'recall': 0.7832618025751072, 'f1': 0.7448979591836734, 'accuracy': 0.9768120393120393}\n"]}],"source":["metric = load_metric(\"seqeval\")\n","results = metric.compute(predictions=predicted_ner, references=true_ner)\n","print({\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]})"]},{"cell_type":"markdown","metadata":{"id":"b6o-_eVNtqCC"},"source":["# **Seqeval Classification Report on Test Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajpF-2FEtqCD"},"outputs":[],"source":["from seqeval.metrics import classification_report\n","results_classification = classification_report(true_ner, predicted_ner)\n","print(results_classification)"]},{"cell_type":"markdown","metadata":{"id":"xmhCwnyatqCD"},"source":["# **FINE TUNING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1JGm9juilT8F"},"outputs":[],"source":["learning_rates = [1e-6, 1e-5, 1e-4]\n","batch_sizes = [6, 10, 16, 32, 64]\n","dropouts = [0.1, 0.2, 0.3, 0.4, 0.5]\n","num_epochs = [2, 4, 6, 8, 10]"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"okcvCn35tqCD"},"outputs":[],"source":["%%time\n","trial = 0\n","\n","best_f1 = 0\n","best_hyperparameters = None\n","\n","# Open the file for writing\n","with open(\"grid_search_results_2.txt\", \"a\") as file:\n","    for learning_rate in learning_rates:\n","        for batch_size in batch_sizes:\n","            batch_train_dataset = DataLoader(train_dataset, batch_size=batch_size)\n","            batch_test_dataset = DataLoader(test_dataset, batch_size=batch_size)\n","            for dropout in dropouts:\n","                    for n_epoch in num_epochs:\n","                            model = BERT_CRF(dropout).to(device)\n","                            # Training loop\n","                            optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","                            model.train()\n","                            for epoch in range(n_epoch):\n","                                print(f'n_epoch {n_epoch}, learning_rate {learning_rate}, batch_size {batch_size}, dropout {dropout}, epoch {epoch}')\n","                                for batch in batch_train_dataset:\n","                                    optimizer.zero_grad()\n","                                    # change the shape and make the batch size as the first dimension\n","                                    input_ids = torch.stack(batch[\"input_ids\"], dim=0).t().to(device)\n","\n","                                    attention_mask = torch.stack(batch[\"attention_mask\"], dim=0).t().to(device)\n","                                    labels = torch.stack(batch[\"labels\"], dim=0).t().to(device)\n","                                    labels_mask = torch.stack(batch[\"label_mask\"], dim=0).t().to(device)\n","                                    loss = model(input_ids, attention_mask, labels_mask, labels)\n","                                    loss.backward()\n","                                    optimizer.step()\n","                                print('loss:',loss.item())\n","                                # Evaluate the model\n","                            true_labels, predicted_labels = evaluate_model(model, batch_test_dataset, device)\n","                            predicted_ner = [[config.id2label[label] for label in sample] for sample in predicted_labels]\n","                            true_ner = [[config.id2label[sample[j]] for j in range(len(sample))] for sample in true_labels]\n","                            eval_results = metric.compute(predictions=predicted_ner, references=true_ner)\n","                            results_classification = classification_report(true_ner, predicted_ner)\n","\n","\n","\n","\n","                        # Check if current F1 is the best\n","                            if eval_results['overall_f1'] > best_f1:\n","                                best_f1 = eval_results['overall_f1']\n","                                best_hyperparameters = {'learning_rate': learning_rate, 'batch size': batch_size, 'dropout': dropout, 'number of epochs': n_epoch}\n","\n","                        # print results\n","                            print(f'Trial {trial + 1}')\n","                            print(f'learning rate: {learning_rate}, batch size: {batch_size}, dropout: {dropout}, number of epochs: {n_epoch}')\n","                            print(f\"F1 Score: {eval_results['overall_f1']}\")\n","                            print(f\"precision: {eval_results['overall_precision']}\")\n","                            print(f\"recall: {eval_results['overall_recall']}\")\n","                            print(results_classification)\n","\n","                        # Write results to the file\n","                            file.write(f'Trial {trial + 1}\\n')\n","                            file.write(f'learning rate: {learning_rate}, batch size: {batch_size}, dropout: {dropout}, number of epochs: {n_epoch}\\n')\n","                            file.write(f\"F1 Score: {eval_results['overall_f1']}\\n\")\n","                            file.write(f\"precision: {eval_results['overall_precision']}\\n\")\n","                            file.write(f\"recall: {eval_results['overall_recall']}\\n\")\n","                            file.write('-' * 80 + '\\n')\n","                            file.flush()\n","\n","                            trial += 1\n","\n","                            del model\n","                            torch.cuda.empty_cache()\n","                            gc.collect()\n","\n","    # Write the best results to the file\n","    file.write('-' * 80 + '\\n')\n","    file.write(f\"Best F1 Score: {best_f1}\\n\")\n","    file.write(f\"Best Hyperparameters: {best_hyperparameters}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"tS27_rJUtqCD"},"source":["# **Custom Metrics**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsGw1CeltqCD"},"outputs":[],"source":["def reformat_predictions(predictions, text):\n","    reformatted_predictions = []\n","    current_type = None\n","    current_start = None\n","    current_end = None\n","    current_entity = None\n","\n","    for entity in predictions:\n","        entity_type = entity['entity']\n","        word = entity['word']\n","        if word.startswith('##') and entity_type.startswith('B-'):\n","            entity['entity'] = entity_type.replace('B-', 'I-')\n","\n","    for entity in predictions:\n","        entity_type = entity['entity']\n","        if entity_type.startswith('B-'):\n","            if current_entity:\n","                reformatted_predictions.append({\n","                    'entity_type': current_type,\n","                    'word': current_entity,\n","                    'start': current_start,\n","                    'end': current_end\n","                })\n","            current_type = entity_type[2:]\n","            current_start = entity['start']\n","            current_end = entity['end']\n","            current_entity = text[current_start:current_end]\n","        elif entity_type.startswith('I-') and current_type == entity_type[2:]:\n","            current_end = entity['end']\n","            current_entity = text[current_start:current_end]\n","    if current_entity:\n","        reformatted_predictions.append({\n","            'entity_type': current_type,\n","            'word': current_entity,\n","            'start': current_start,\n","            'end': current_end\n","        })\n","\n","    return reformatted_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1g74RKgxtqCD"},"outputs":[],"source":["all_entities = []\n","\n","for i in range(len(test)):\n","    lst = []\n","    text, entities = test[i]\n","    pred = nlp(text)\n","    pred = reformat_predictions(pred, text)\n","    for item in pred:\n","        lst.append((item['word'], item['entity_type']))\n","    all_entities.append(lst)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvkAXxmGtqCD"},"outputs":[],"source":["def compute_confusion_matrix(pred, truth):\n","    \"\"\"Compute TP, FP, and FN.\"\"\"\n","    truth = set(truth)\n","    pred = set(pred)\n","\n","    TP = len(truth.intersection(pred))\n","    FP = len(pred - truth)\n","    FN = len(truth - pred)\n","    return TP, FP, FN\n","\n","def compute_metrics_post_training(TP, FP, FN):\n","    \"\"\"Compute precision, recall, and F1 score.\"\"\"\n","    precision = TP / (TP + FP) if TP + FP > 0 else 0\n","    recall = TP / (TP + FN) if TP + FN > 0 else 0\n","    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n","    return precision, recall, f1\n","\n","metrics = {\n","    'PER': {'TP': 0, 'FP': 0, 'FN': 0},\n","    'ORG': {'TP': 0, 'FP': 0, 'FN': 0},\n","    'TOTAL': {'TP': 0, 'FP': 0, 'FN': 0}\n","}\n","\n","for i in range(len(test)):\n","    entities_pred = all_entities[i]\n","    text, entities_truth = test[i]\n","\n","    for tag in ['PER', 'ORG']:\n","        pred_set = set()\n","        truth_set = set()\n","        for entity, label in entities_pred:\n","            entity = entity.lower()\n","            if label == tag:\n","                pred_set.add(entity)\n","        for entity in entities_truth:\n","            label, start, end = entity\n","            if label == tag:\n","                truth_set.add(text[start:end].lower())\n","\n","        TP, FP, FN = compute_confusion_matrix(pred_set, truth_set)\n","\n","        metrics[tag]['TP'] += TP\n","        metrics[tag]['FP'] += FP\n","        metrics[tag]['FN'] += FN\n","\n","        metrics['TOTAL']['TP'] += TP\n","        metrics['TOTAL']['FP'] += FP\n","        metrics['TOTAL']['FN'] += FN\n","\n","for tag, values in metrics.items():\n","    precision, recall, f1 = compute_metrics_post_training(values['TP'], values['FP'], values['FN'])\n","    print(f\"{tag} -- Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i158HyWptqCD"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}