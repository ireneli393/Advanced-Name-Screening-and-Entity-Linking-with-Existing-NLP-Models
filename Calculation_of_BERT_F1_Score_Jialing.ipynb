{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Load Data**"],"metadata":{"id":"P0Tr1s-lHPl4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"stuyKkz9MyyO"},"outputs":[],"source":["!pip install transformers\n","from transformers import pipeline\n","import json\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, BertTokenizerFast, BertForTokenClassification"]},{"cell_type":"markdown","source":["**Read File**"],"metadata":{"id":"un-BVgK3HBbk"}},{"cell_type":"code","source":["def read_dataset(file):\n","  with open(file, \"r\") as json_file:\n","    data = json.load(json_file)\n","    train = data['train']\n","    test = data['test']\n","  return train, test\n","\n","file = \"label_data_train_test.json\"\n","train, test = read_dataset(file)"],"metadata":{"id":"XvSLwOwkNOwo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Build Model**"],"metadata":{"id":"QyczHU9aHYqw"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForTokenClassification\n","from transformers import pipeline\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n","model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n","nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"],"metadata":{"id":"Pg_LYB3FNqCU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ner_results = []\n","for text, labels in train:\n","  res = nlp(text)\n","  filter = [item for item in res if 'ORG' in item.get('entity', '') or 'PER' in item.get('entity', '')]\n","  ner_results.append(filter)"],"metadata":{"id":"6n_5LIgZNWqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Post-Processing: Align prediction words and Labels**"],"metadata":{"id":"ZuhwqPZcHbHo"}},{"cell_type":"code","source":["def align_prediction(nonsplit_ner_results):\n","    aligned_predictions = []\n","    current_word = []\n","    current_entity = None\n","    for token in nonsplit_ner_results:\n","      word = token['word']\n","      entity = token['entity']\n","      if entity.startswith('B-'):\n","          if current_word:\n","              aligned_predictions.append({'word': ' '.join(current_word).replace(' ##', '').replace('##', ''), 'entity': current_entity})\n","          current_word = []\n","          current_entity = entity[2:]\n","      current_word.append(word)\n","    if current_word:\n","      aligned_predictions.append({'word': ' '.join(current_word).replace(' ##', '').replace('##', ''), 'entity': current_entity})\n","    return aligned_predictions\n","predicted_labels = []\n","for i in range(len(ner_results)):\n","  predicted_labels.append(align_prediction(ner_results[i]))\n"],"metadata":{"id":"AIrcVp-eUNHQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Example Illustrationï¼š If the model tokenizes one word in two parts and consider them as two different words, then we don't combine them back to their original form\n","\n","\n","*   below: 'An', 'ubha' and 'v Poddar' are considered as three words\n","*   while in the ground truth, they consist of one word\n","\n"],"metadata":{"id":"T10CVWy-AYVS"}},{"cell_type":"code","source":["for item in ner_results[1]:\n","  if 'PER' in item['entity']:\n","\n","    print([item['entity'], item['word'],item['start'],item['end']])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NG5zJ3xd6vKN","outputId":"b99a3cf1-81e1-468c-ea7a-14afce1e5c57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['B-PER', 'An', 434, 436]\n","['B-PER', '##ub', 436, 438]\n","['I-PER', '##ha', 438, 440]\n","['B-PER', '##v', 440, 441]\n","['I-PER', 'Po', 442, 444]\n","['I-PER', '##dd', 444, 446]\n","['I-PER', '##ar', 446, 448]\n","['B-PER', 'Ash', 450, 453]\n","['B-PER', '##ish', 453, 456]\n","['I-PER', 'Singh', 457, 462]\n","['I-PER', '##ania', 462, 466]\n"]}]},{"cell_type":"code","source":["predicted_labels[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grK-K7vgc4wS","outputId":"8ea3fb31-3925-483f-d66f-2ce3b5ba4628"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'word': 'ETRON', 'entity': 'ORG'},\n"," {'word': 'Tetron Commercial Ltd & Services', 'entity': 'ORG'},\n"," {'word': 'Tetron Commercial Ltd', 'entity': 'ORG'},\n"," {'word': 'Tetron Commercial Ltd', 'entity': 'ORG'},\n"," {'word': 'An', 'entity': 'PER'},\n"," {'word': 'ubha', 'entity': 'PER'},\n"," {'word': 'v Poddar', 'entity': 'PER'},\n"," {'word': 'Ash', 'entity': 'PER'},\n"," {'word': 'ish Singhania', 'entity': 'PER'},\n"," {'word': 'Tetron Commercial Ltd', 'entity': 'ORG'},\n"," {'word': 'Tetron Commercial Ltd', 'entity': 'ORG'},\n"," {'word': 'Tetron Commercial Ltd', 'entity': 'ORG'}]"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["true_labels = []\n","for text, labels in train:\n","    result = []\n","    for ner, start, end in labels:\n","        word = text[start:end]\n","        result.append([word, ner])\n","\n","    true_labels.append(result)\n","true_labels[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHBdWPyMco_F","outputId":"29feaa72-db22-47f8-b856-aeca4d399ec4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['TETRON COMMERCIAL LTD', 'ORG'],\n"," ['Tetron Commercial Ltd', 'ORG'],\n"," ['Tetron Commercial Ltd', 'ORG'],\n"," ['Tetron Commercial Ltd', 'ORG'],\n"," ['Anubhav Poddar', 'PERSON'],\n"," ['Ashish Singhania', 'PERSON'],\n"," ['Tetron Commercial Ltd', 'ORG'],\n"," ['Tetron Commercial Ltd', 'ORG'],\n"," ['Tetron Commercial Ltd', 'ORG']]"]},"metadata":{},"execution_count":55}]},{"cell_type":"markdown","source":["**Calculate Metrics**"],"metadata":{"id":"hfSTtR_IHlCp"}},{"cell_type":"markdown","source":["True positives only when the predicted word & label pair are the exactly same as the corresponding groud truth pair"],"metadata":{"id":"-alCeGNTBh-l"}},{"cell_type":"code","source":["def calculate_metrics(predicted_labels, ground_truth_labels, num):\n","    # Initialize variables to count true positives, false positives, and false negatives\n","    true_positives, false_positives, false_negatives = 0, 0, 0\n","    # Iterate through each datapoint and calculate true positives, false positives, and false negatives\n","    for i in range(num):\n","        predicted_sublabels = [(label['word'], label['entity']) for label in predicted_labels[i]]\n","        ground_truth_sublabels = [(label[0], label[1]) for label in ground_truth_labels[i]]\n","\n","        sub_true_positives = 0\n","        sub_false_positives = 0\n","        sub_false_negatives = len(ground_truth_sublabels)\n","\n","        for label in predicted_sublabels:\n","            if label in ground_truth_sublabels:\n","                sub_true_positives += 1\n","            else:\n","                sub_false_positives += 1\n","        sub_false_negatives -= sub_true_positives\n","\n","        true_positives += sub_true_positives\n","        false_positives += sub_false_positives\n","        false_negatives += sub_false_negatives\n","\n","    # Calculate precision, recall, and F1 score\n","    precision = true_positives / (true_positives + false_positives)\n","    recall = true_positives / (true_positives + false_negatives)\n","    f1_score = 2 * (precision * recall) / (precision + recall)\n","\n","    return precision, recall, f1_score\n","\n","# Calculate precision, recall, and F1 score\n","precision, recall, f1_score = calculate_metrics(predicted_labels, true_labels, len(train))\n","\n","# Print the results\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1 Score:\", f1_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cI57x_nljp-F","outputId":"3fec3c87-d2a8-4b6c-d973-7332319a744c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.23613690634120457\n","Recall: 0.1319887290523506\n","F1 Score: 0.16933028919330292\n"]}]}]}